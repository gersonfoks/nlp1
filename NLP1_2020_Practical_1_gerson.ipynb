{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIZrAUx57vsM"
   },
   "source": [
    "Practical 1: Sentiment Detection in Movie Reviews\n",
    "========================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4kXPMhyngZW"
   },
   "source": [
    "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
    "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
    "Each review is a **document** and consists of one or more sentences.\n",
    "\n",
    "To prepare yourself for this practical, you should\n",
    "have a look at a few of these texts to understand the difficulties of\n",
    "the task: how might one go about classifying the texts? You will write\n",
    "code that decides whether a movie review conveys positive or\n",
    "negative sentiment.\n",
    "\n",
    "Please make sure you have read the following paper:\n",
    "\n",
    ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
    "(2002). \n",
    "[Thumbs up? Sentiment Classification using Machine Learning\n",
    "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
    "\n",
    "Bo Pang et al. were the \"inventors\" of the movie review sentiment\n",
    "classification task, and the above paper was one of the first papers on\n",
    "the topic. The first version of your sentiment classifier will do\n",
    "something similar to Pang et al.'s system. If you have questions about it,\n",
    "you should resolve as soon as possible with your TA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb7errgRASzZ"
   },
   "source": [
    "**Advice**\n",
    "\n",
    "Please read through the entire practical and familiarise\n",
    "yourself with all requirements before you start coding or otherwise\n",
    "solving the tasks. Writing clean and concise code can make the difference\n",
    "between solving the assignment in a matter of hours, and taking days to\n",
    "run all experiments.\n",
    "\n",
    "## Environment\n",
    "\n",
    "All code should be written in **Python 3**. \n",
    "This is the default in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SaZnxptMJiD7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYZyIF7lJnGn"
   },
   "source": [
    "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
    "The easiest way to\n",
    "install Python is through downloading\n",
    "[Anaconda](https://www.anaconda.com/download). \n",
    "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
    "You can also use an IDE\n",
    "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
    "coding and debugging easier. It is good practice to create a [virtual\n",
    "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
    "project, so that any Python packages don’t interfere with other\n",
    "projects. \n",
    " \n",
    "\n",
    "**Learning Python 3**\n",
    "\n",
    "If you are new to Python 3, you may want to check out a few of these resources:\n",
    "- https://learnxinyminutes.com/docs/python3/\n",
    "- https://www.learnpython.org/\n",
    "- https://docs.python.org/3/tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hok-BFu9lGoK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "from subprocess import call\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import sklearn as sk\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXWyGHwE-ieQ"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "**Download the sentiment lexicon and the movie reviews dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sent_lexicon', <http.client.HTTPMessage at 0x2a176b57048>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = 'https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6'\n",
    "filename = 'sent_lexicon'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('reviews.json', <http.client.HTTPMessage at 0x2a176b57b88>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760'\n",
    "filename = 'reviews.json'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lm-rakqtlMOT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# download sentiment lexicon\n",
    "!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
    "# download review data\n",
    "!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkPwuHp5LSuQ"
   },
   "source": [
    "**Load the movie reviews.**\n",
    "\n",
    "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "careEKj-mRpl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 2000 \n",
      "\n",
      "0 NEG 29\n",
      "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
      "1 NEG 11\n",
      "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
      "2 NEG 24\n",
      "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
      "3 NEG 19\n",
      "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
      "4 NEG 38\n",
      "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
      "\n",
      "Number of word types: 47743\n",
      "Number of word tokens: 1512359\n",
      "\n",
      "Most common tokens:\n",
      "         , :    77842\n",
      "       the :    75948\n",
      "         . :    59027\n",
      "         a :    37583\n",
      "       and :    35235\n",
      "        of :    33864\n",
      "        to :    31601\n",
      "        is :    25972\n",
      "        in :    21563\n",
      "        's :    18043\n",
      "        it :    15904\n",
      "      that :    15820\n",
      "     -rrb- :    11768\n",
      "     -lrb- :    11670\n",
      "        as :    11312\n",
      "      with :    10739\n",
      "       for :     9816\n",
      "       his :     9542\n",
      "      this :     9497\n",
      "      film :     9404\n"
     ]
    }
   ],
   "source": [
    "# file structure:\n",
    "# [\n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
    "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
    "#   ..\n",
    "# ]\n",
    "# where `content` is a list of sentences, \n",
    "# with a sentence being a list of (token, pos_tag) pairs.\n",
    "\n",
    "\n",
    "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "  reviews = json.load(f)\n",
    "  \n",
    "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
    "\n",
    "def print_sentence_with_pos(s):\n",
    "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
    "\n",
    "for i, r in enumerate(reviews):\n",
    "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
    "  print_sentence_with_pos(r[\"content\"][0])\n",
    "  if i == 4: \n",
    "    break\n",
    "    \n",
    "c = Counter()\n",
    "for review in reviews:\n",
    "  for sentence in review[\"content\"]:\n",
    "    for token, pos_tag in sentence:\n",
    "      c[token.lower()] += 1\n",
    "      \n",
    "print(\"\\nNumber of word types:\", len(c))\n",
    "print(\"Number of word tokens:\", sum(c.values()))\n",
    "\n",
    "print(\"\\nMost common tokens:\")\n",
    "for token, count in c.most_common(20):\n",
    "  print(\"%10s : %8d\" % (token, count))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6PWaEoh8B34"
   },
   "source": [
    "# Lexicon-based approach (2+1pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsTSMb6ma4E8"
   },
   "source": [
    "A traditional approach to automatically classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative) or a sentiment score (e.g., a score from 0 to 5).\n",
    "\n",
    "In this practical, you will use the sentiment\n",
    "lexicon released by Wilson et al. (2005). The path of the loaded lexicon is `\"sent_lexicon\"`.\n",
    "\n",
    "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
    "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
    "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
    "\n",
    "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCHUMmb0SZxy"
   },
   "source": [
    "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ogq0Eq2hQglh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abash pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abate pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=weaksubj len=1 word1=abdicate pos1=verb stemmed1=y priorpolarity=negative\n",
      "type=strongsubj len=1 word1=aberration pos1=adj stemmed1=n priorpolarity=negative\n",
      "type=strongsubj len=1 word1=aberration pos1=noun stemmed1=n priorpolarity=negative\n",
      "type=strongsubj len=1 word1=abhor pos1=anypos stemmed1=y priorpolarity=negative\n"
     ]
    }
   ],
   "source": [
    "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "  line_cnt = 0\n",
    "  for line in f:\n",
    "    print(line.strip())\n",
    "    line_cnt += 1\n",
    "    if line_cnt > 10:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mml4nOtIUBhn"
   },
   "source": [
    "Given such a sentiment lexicon, there are ways to solve\n",
    "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
    "$S_{binary}$ by counting how many words have a positive or a\n",
    "negative label in the sentiment lexicon $SLex$.\n",
    "\n",
    "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
    "\n",
    "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
    "\n",
    "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
    "\n",
    "$$\n",
    "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
    "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
    "        \\text{negative} & \\text{otherwise}\n",
    "        \\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOFnMvbeeZrc"
   },
   "source": [
    "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def content_to_sentence(content):\n",
    "    '''\n",
    "        Creates a sentence (list) of the content of a review\n",
    "        content: [(word: string, POS: string)]\n",
    "        returns: [word: string]\n",
    "    '''\n",
    "    whole_review = []\n",
    "    for sentence in content:\n",
    "        whole_review += [word.lower() for word in np.array(sentence)[:, 0]]\n",
    "    \n",
    "    return whole_review\n",
    "\n",
    "def create_dataset(reviews):\n",
    "    '''\n",
    "        Creates the dataset given the reviews\n",
    "        Splits it up in sentences and the target class:\n",
    "        returns: sentences: [[word: string]], [int] \n",
    "    '''\n",
    "    targets = []\n",
    "    sentences = []\n",
    "    \n",
    "    ### Loop over the reviews, get the target, and make 1 sentence of the content. \n",
    "    for review in reviews:\n",
    "        targets.append(int( review['sentiment'] == 'POS'))\n",
    "        \n",
    "        whole_review = content_to_sentence(review['content'])\n",
    "        sentences.append(whole_review)\n",
    "    return sentences, targets\n",
    "\n",
    "sentences, targets = create_dataset(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ED2aTEYutW1-"
   },
   "outputs": [],
   "source": [
    "### Load the lexixon in a default dict\n",
    "from collections import defaultdict\n",
    "\n",
    "## The possible sentiments\n",
    "sentiments = {\n",
    "    'positive': 1,\n",
    "    'negative': -1,\n",
    "    'both': 0,\n",
    "    'neutral': 0\n",
    "}\n",
    "\n",
    "def get_slex(ref=\"sent_lexicon\"):\n",
    "    '''\n",
    "    Creates the sentiment lexicon\n",
    "    ref: reference of the file \n",
    "    returns: Defaultdictionary{word:  sentiment score} \n",
    "    \n",
    "    '''\n",
    "    slex = defaultdict(float)\n",
    "    \n",
    "    ## indexes\n",
    "    key_index = 2\n",
    "    key_strip = \"word1=\"\n",
    "    value_index = 5\n",
    "    value_strip = \"priorpolarity=\"\n",
    "    \n",
    "    magnitude_index = 0\n",
    "    magnitude_strip = \"type=\"\n",
    "    \n",
    "    magnitude_choices = defaultdict(float, {\n",
    "        \"weaksubj\": 1.0,\n",
    "        \"strongsubj\": 2.0\n",
    "    })\n",
    "    \n",
    "    with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "        line_cnt = 0\n",
    "        for line in f:\n",
    "            line_cnt += 1\n",
    "            line_arr = line.split()\n",
    "            magnitude = magnitude_choices[ line_arr[magnitude_index].replace(magnitude_strip, \"\")]\n",
    "            sentiment = sentiments[line_arr[value_index].replace(value_strip, \"\")]\n",
    "            slex[line_arr[key_index].replace(key_strip, \"\")] =  magnitude * sentiment\n",
    "    return slex\n",
    "\n",
    "\n",
    "    \n",
    "slex = get_slex()\n",
    "\n",
    "\n",
    "class BinarySentimentClassifier:\n",
    "    '''\n",
    "        The binary classifier, that uses a threshold and the binary score to classify a review\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, slex, threshold):\n",
    "        self.slex = slex\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        return [int( self.binary_sentiment(sentence) > self.threshold) for sentence in sentences ]\n",
    "    \n",
    "    def binary_sentiment(self, sentence):\n",
    "        '''\n",
    "        sentence: sentence to calculate the sentiment from\n",
    "        returns the binary sentiment score of a sentence given the sentiment lexicon\n",
    "        '''\n",
    "        return np.sum([ np.sign( self.slex[word])   for word in sentence])\n",
    "\n",
    "\n",
    "\n",
    "# Create the classifier:\n",
    "bin_sent_model = BinarySentimentClassifier(slex, 8)\n",
    "\n",
    "predictions = bin_sent_model.predict(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_results(predictions, targets):\n",
    "    '''\n",
    "        prediction: predictions of a model\n",
    "        targets: the ground truth\n",
    "        returns an array containing a 0 if the prediction is incorrect and 1 otherwise\n",
    "    '''\n",
    "    return np.array(predictions) == np.array(targets)\n",
    "\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    '''\n",
    "        Calculates the accuracy of the predictions\n",
    "    '''\n",
    "    return np.sum(calculate_results(predictions, targets)) / len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iy528EUTphz5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "# token_results should be a list of binary indicators; for example [1, 0, 1, ...] \n",
    "# where 1 indicates a correct classification and 0 an incorrect classification.\n",
    "token_results = calculate_results(predictions, targets)\n",
    "token_accuracy = calculate_accuracy(predictions, targets)\n",
    "print(\"Accuracy: %0.2f\" % token_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Twox0s_3eS0V"
   },
   "source": [
    "As the sentiment lexicon also has information about the **magnitude** of\n",
    "sentiment (e.g., *“excellent\"* would have higher magnitude than\n",
    "*“good\"*), we can take a more fine-grained approach by adding up all\n",
    "sentiment scores, and deciding the polarity of the movie review using\n",
    "the sign of the weighted score $S_{weighted}$.\n",
    "\n",
    "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
    "\n",
    "\n",
    "Make sure you define an appropriate threshold for this approach.\n",
    "\n",
    "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.8195\n"
     ]
    }
   ],
   "source": [
    "### To calculate the bias we calculate the avarage sentiment of each review. \n",
    "### Reviews that have a higher sentiment then the avarage will get a positive label, and negative label otherwise\n",
    "avg = 0\n",
    "total = 0\n",
    "for sentence in sentences:\n",
    "    total += np.sum([ slex[word]   for word in sentence])\n",
    "avg = total / len(sentences)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qG3hUDnPtkhS"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MagnitudeClassifier:\n",
    "    \n",
    "    def __init__(self, slex, threshold):\n",
    "        self.slex = slex\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        return [int( self.binary_sentiment(sentence) > self.threshold) for sentence in sentences ]\n",
    "    \n",
    "    def binary_sentiment(self, sentence):\n",
    "        '''\n",
    "        sentence: sentence to calculate the sentiment from\n",
    "        returns the binary sentiment score of a sentence given the sentiment lexicon\n",
    "        '''\n",
    "\n",
    "        return np.sum([ self.slex[word]   for word in sentence])\n",
    "\n",
    "magnitude_model = MagnitudeClassifier(slex, avg)\n",
    "\n",
    "predictions = magnitude_model.predict(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9vVk7CvDpyka"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n"
     ]
    }
   ],
   "source": [
    "magnitude_results = calculate_results(predictions, targets)\n",
    "magnitude_accuracy = calculate_accuracy(predictions, targets)\n",
    "print(\"Accuracy: %0.2f\" % magnitude_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9SHoGPfsAHV"
   },
   "source": [
    "#### Make a barplot of the two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8LgBcYcXsEk3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAASFElEQVR4nO3df9Sfd13f8edrdxpbugGlucGZpKZHgywoML0Xth2nIHYnFTAizCWAXTcxC2dxcrbuEHc8nGqnrrDxQ20XclgEPGpAAc0gUGZni6MWk7o2NIVgFiq9qY6bwqkrU9uU9/74XtGrN9/7/l53+r2b9uPzcc73cH1+3Nf1Dt8rr17fz/29rqSqkCQ98f2Nc12AJGk6DHRJaoSBLkmNMNAlqREGuiQ1Ys25OvC6detq06ZN5+rwkvSEdNttt32xqmbHjZ2zQN+0aRNHjx49V4eXpCekJH+01JhLLpLUCANdkhphoEtSIwYFepJtSU4kOZlk75jxf5fk9u51Z5KHkzxt+uVKkpYyMdCTzADXAZcDW4CdSbb051TVm6rqeVX1POAngJur6kurUK8kaQlDrtC3Aier6lRVPQgcBLYvM38n8GvTKE6SNNyQQF8P3NNrz3d9XyPJk4BtwPsefWmSpJUYEugZ07fUM3dfCnx8qeWWJLuSHE1ydGFhYWiNkqQBhgT6PLCx194A3LvE3B0ss9xSVfuraq6q5mZnx97oJEk6S0PuFD0CbE5yKfB5RqH9ysWTkjwF+G7g1VOtUHqC2rT3Q+e6BD1O3f0fX7wq+50Y6FV1Oske4AZgBjhQVceT7O7G93VTXwZ8tKq+siqVSpKWNehZLlV1GDi8qG/fovY7gXdOqzBJ0sp4p6gkNeKcPW3x0XBtUstZrfVJ6fHOK3RJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEYMCPcm2JCeSnEyyd4k5L0hye5LjSW6ebpmSpEkm/iPRSWaA64DLgHngSJJDVXVXb85TgeuBbVX1uSRPX6V6JUlLGHKFvhU4WVWnqupB4CCwfdGcVwLvr6rPAVTVF6ZbpiRpkiGBvh64p9ee7/r6nglclOSmJLcluWJaBUqShpm45AJkTF+N2c93AC8CLgB+L8mtVfWZR+wo2QXsArjkkktWXq0kaUlDrtDngY299gbg3jFzPlJVX6mqLwIfA567eEdVtb+q5qpqbnZ29mxrliSNMSTQjwCbk1yaZC2wAzi0aM5vAf8oyZokTwKeD3xquqVKkpYzccmlqk4n2QPcAMwAB6rqeJLd3fi+qvpUko8Ax4CvAu+oqjtXs3BJ0iMNWUOnqg4Dhxf17VvUfhPwpumVJklaCe8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWrEoEBPsi3JiSQnk+wdM/6CJPcnub17vWH6pUqSlrNm0oQkM8B1wGXAPHAkyaGqumvR1N+tqpesQo2SpAGGXKFvBU5W1amqehA4CGxf3bIkSSs1JNDXA/f02vNd32L/IMkdST6c5NnjdpRkV5KjSY4uLCycRbmSpKUMCfSM6atF7T8AvrGqngv8AvCb43ZUVfuraq6q5mZnZ1dUqCRpeUMCfR7Y2GtvAO7tT6iqP62qB7rtw8B5SdZNrUpJ0kRDAv0IsDnJpUnWAjuAQ/0JSb4+Sbrtrd1+75t2sZKkpU38lktVnU6yB7gBmAEOVNXxJLu78X3AK4DXJjkN/Bmwo6oWL8tIklbRxECHv1xGObyob19v+xeBX5xuaZKklfBOUUlqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGDAr0JNuSnEhyMsneZeb9vSQPJ3nF9EqUJA0xMdCTzADXAZcDW4CdSbYsMe9a4IZpFylJmmzIFfpW4GRVnaqqB4GDwPYx834MeB/whSnWJ0kaaEigrwfu6bXnu76/lGQ98DJg33I7SrIrydEkRxcWFlZaqyRpGUMCPWP6alH7rcDrq+rh5XZUVfuraq6q5mZnZweWKEkaYs2AOfPAxl57A3DvojlzwMEkAOuA70tyuqp+cxpFSpImGxLoR4DNSS4FPg/sAF7Zn1BVl57ZTvJO4IOGuSQ9tiYGelWdTrKH0bdXZoADVXU8ye5ufNl1c0nSY2PIFTpVdRg4vKhvbJBX1ZWPvixJ0kp5p6gkNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwYFepJtSU4kOZlk75jx7UmOJbk9ydEk3zn9UiVJy1kzaUKSGeA64DJgHjiS5FBV3dWbdiNwqKoqyXOA9wLPWo2CJUnjDblC3wqcrKpTVfUgcBDY3p9QVQ9UVXXNC4FCkvSYGhLo64F7eu35ru8RkrwsyaeBDwH/YtyOkuzqlmSOLiwsnE29kqQlDAn0jOn7mivwqvpAVT0L+AHgmnE7qqr9VTVXVXOzs7MrKlSStLwhgT4PbOy1NwD3LjW5qj4GfFOSdY+yNknSCgwJ9CPA5iSXJlkL7AAO9Sck+eYk6ba/HVgL3DftYiVJS5v4LZeqOp1kD3ADMAMcqKrjSXZ34/uAlwNXJHkI+DPgn/Z+SSpJegxMDHSAqjoMHF7Ut6+3fS1w7XRLkySthHeKSlIjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDViUKAn2ZbkRJKTSfaOGX9VkmPd65Ykz51+qZKk5UwM9CQzwHXA5cAWYGeSLYumfRb47qp6DnANsH/ahUqSljfkCn0rcLKqTlXVg8BBYHt/QlXdUlVf7pq3AhumW6YkaZIhgb4euKfXnu/6lvIjwIfHDSTZleRokqMLCwvDq5QkTTQk0DOmr8ZOTF7IKNBfP268qvZX1VxVzc3Ozg6vUpI00ZoBc+aBjb32BuDexZOSPAd4B3B5Vd03nfIkSUMNuUI/AmxOcmmStcAO4FB/QpJLgPcDP1xVn5l+mZKkSSZeoVfV6SR7gBuAGeBAVR1Psrsb3we8AbgYuD4JwOmqmlu9siVJiw1ZcqGqDgOHF/Xt622/BnjNdEuTJK2Ed4pKUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNWJQoCfZluREkpNJ9o4Zf1aS30vyF0mumn6ZkqRJ1kyakGQGuA64DJgHjiQ5VFV39aZ9CfjXwA+sRpGSpMmGXKFvBU5W1amqehA4CGzvT6iqL1TVEeChVahRkjTAkEBfD9zTa893fSuWZFeSo0mOLiwsnM0uJElLGBLoGdNXZ3OwqtpfVXNVNTc7O3s2u5AkLWFIoM8DG3vtDcC9q1OOJOlsDQn0I8DmJJcmWQvsAA6tblmSpJWa+C2XqjqdZA9wAzADHKiq40l2d+P7knw9cBR4MvDVJK8DtlTVn65e6ZKkvomBDlBVh4HDi/r29bb/hNFSjCTpHPFOUUlqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGDAr0JNuSnEhyMsneMeNJ8vPd+LEk3z79UiVJy5kY6ElmgOuAy4EtwM4kWxZNuxzY3L12Af9lynVKkiYYcoW+FThZVaeq6kHgILB90ZztwLtr5FbgqUn+9pRrlSQtY82AOeuBe3rteeD5A+asB/64PynJLkZX8AAPJDmxomq1lHXAF891EY8XufZcV6AxPEd7HuU5+o1LDQwJ9Izpq7OYQ1XtB/YPOKZWIMnRqpo713VIS/EcfWwMWXKZBzb22huAe89ijiRpFQ0J9CPA5iSXJlkL7AAOLZpzCLii+7bL3wfur6o/XrwjSdLqmbjkUlWnk+wBbgBmgANVdTzJ7m58H3AY+D7gJPD/gH++eiVrDJex9HjnOfoYSNXXLHVLkp6AvFNUkhphoEtSIwz0gZJcnOT27vUnST7fa69dNPfuJOtWoYbZJA8l+ZfT3rcen5JUkl/utdckWUjywVU41u4kV3TbVyb5hrPYx4rPfc/r6THQB6qq+6rqeVX1PGAf8JYz7e4O2sfCPwFuBXau5kGSDLk/QY+NrwDfmuSCrn0Z8PnVOFBV7auqd3fNK4EVB/pZ8ryeEgP9UUjyoiT/K8knkxxI8nWLxi9I8pEkP5rkwm7Oke5ntndzrkzy/m7eHyZ54zKH3An8W2BDkvW941zRPRTtjjNXc0mekeQDXd8dSf5hkk1J7uz93FVJru62b0rys0luBn48yUuTfKKr9beTPKOb9zeT/FL3Zz6W5OVJfiTJW3r7/dEkb37U/wfrjA8DL+62dwK/dmYgydYkt3Tv0y1JvqXrf1KS93bv0Xu693KuG3sgyc9058Wtvff26u6ceAUwB/xK9wn0gv6Vd5K5JDd12xcn+Wh3/LfTu8kwyauT/H63j7dn9FyocTyvp6WqfK3wBVwN/CSjxx08s+t7N/C6bvtuYBPw28AVXd/PAq/utp8KfAa4kNGV0CngKcD5wB8BG8cccyPwh719/Ztu+9nACWBd135a97/v6dUz0+1/E3Bnb59XAVd32zcB1/fGLuKvvgX1GuA/d9vXAm9dNO9C4H8D53V9twDfdq7fpxZewAPAc4Df6M6P24EXAB/sxp8MrOm2vxd4X++9fXu3/a3AaWCuaxfw0m77jcBP9s7rq3rnw1yvjrt759gccFO3/fPAG7rtF3f7Xgf8HeC/9c6J68/8XfC8Xr1X8x9BVtEM8Nmq+kzXfhfwr4C3du3fAt5YVb/Stf8x8P1Jrura5wOXdNs3VtX9AEnuYvSshv6zcWB0Q9d7u+2DwH8F3gx8D/AbVfVFgKr6Ujfne4Arur6HgfuTXDThz/Se3vYG4D0ZPWRtLfDZrv97u1ro9v3lru7/AbwkyacY/QX45IRjaaCqOpZkE6Mr2cOLhp8CvCvJZkZhel7X/53A27qfvzPJsd7PPAicWYO/jdEyztn6LuAHu+N8KMmXu/4XAd8BHEkCcAHwhTE/73k9RQb62fvKhPGPA5cn+dUa/ec9wMur6hEPJEvyfOAvel0PM/592Qk8I8mruvY3dH+Jw5jn5izhNI9cZjt/0Xj/z/QLwJur6lCSFzC6emOZ470D+PfAp4FfGliPhjsE/CdGV+cX9/qvAX6nql7Whf5NXf+45yud8VB3TsLS59ti/XNn8Xkz7nwI8K6q+okJ+/W8niLX0M/e+cCmJN/ctX8YuLk3/gbgPkYfNWF0p+2PpbtcSfJ3hx6oWxe9sKrWV9WmqtoE/ByjK4obgR9KcnE392ndj90IvLbrm0nyZOD/AE/v1j2/DnjJMod9Cn/1y7d/1uv/KLCnV9tFAFX1CUYfn19Jb41XU3MA+OkxV4j99+nKXv//BH4IIKN/v+DbVni8/wv8rV77bkZX3AAv7/V/DHhVd5zLGS1VwOj8e0WSp3djT0vyiKcEel5Pn4F+9v6c0SMOfj3JJ4GvMvr2S9/rgPMz+kXnNYw+Dh/rfoFzzQqOtRP4wKK+9wE7q+o48DPAzUnuYPRxFeDHgRd2td0GPLuqHgJ+GvgEo4/cn17mmFd3f7bf5ZGPPf0PwEVJ7uyO98Le2HuBj5/5uKrpqar5qnrbmKE3Aj+X5OOMlgHPuB6Y7ZZaXg8cA+5fwSHfCew780tR4KeAt3Xnw8O9eT8FfFeSP2C0rPi5rt67GP2e6aNdDf8dWPxvJHheT5m3/mtqMvpu9Fuq6sZzXctfd903Ss6rqj9P8k2MrmyfWY/dV2yb8UQ6r11D16OW5KnA7wN3PBFO+r8mngT8TpLzGK0Pv9YwX5kn4nntFbokNcI1dElqhIEuSY0w0CWpEQa6JDXCQJekRvx/5E8XA3hVe7YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(['Token Accuracy', 'Magnitude Accuracy'], [token_accuracy, magnitude_accuracy])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNhS8OCVxMHd"
   },
   "source": [
    "## (Q1.3 Optional) A better threshold (1pt)\n",
    "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
    "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xo7gk1I-omLI"
   },
   "source": [
    "Take the case that we have a review of 7 words, in that case we would never get that at least 7 words in the review are positive. \n",
    "Therefore small reviews have a smaller chance of crossing the threshold of having at least 8 more positive words than negative.\n",
    "\n",
    "Normalizing with respect to the length of a review would countaract this problem. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Dwt0B8h8aKjr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009082534363548262\n"
     ]
    }
   ],
   "source": [
    "## calculate the average percentage of the difference in positive and negative words\n",
    "avg = 0\n",
    "total = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    pos_count = 0\n",
    "    neg_count  = 0\n",
    "    for word in sentence:\n",
    "        s = np.sign(slex[word]) \n",
    "\n",
    "        if  s < 0:\n",
    "            neg_count += 1\n",
    "        if s > 0:\n",
    "            pos_count += 1\n",
    "    total += ((pos_count - neg_count) / len(sentence))\n",
    "\n",
    "\n",
    "avg = total/len(sentences) \n",
    "print(avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "class BinarySentimentPercClassifier:\n",
    "    \n",
    "    def __init__(self, slex, threshold):\n",
    "        self.slex = slex\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        return [int( self.binary_sentiment(sentence) > ( self.threshold * len(sentence) )) for sentence in sentences ]\n",
    "    \n",
    "    def binary_sentiment(self, sentence):\n",
    "        '''\n",
    "        sentence: sentence to calculate the sentiment from\n",
    "        returns the binary sentiment score of a sentence given the sentiment lexicon\n",
    "        '''\n",
    "        return np.sum([ np.sign(self.slex[word])   for word in sentence])\n",
    "                    \n",
    "perc_model = BinarySentimentPercClassifier(slex, avg)\n",
    "\n",
    "predictions = perc_model.predict(sentences)\n",
    "perc_results = predictions\n",
    "perc_accuracy = calculate_accuracy(predictions, targets)\n",
    "print(\"Accuracy: %0.2f\" % perc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly, as you can see, using a percentual threshold does not create a better classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MFrz8Jink0D"
   },
   "source": [
    "# Significance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxkxrldT9Ymc"
   },
   "source": [
    "Does using the magnitude improve the results? Oftentimes, answering such questions by simply comparing accuracy\n",
    "scores is not enough. When dealing with natural language and human ratings, it is safe to assume that there are infinitely many possible\n",
    "instances that could be used for training and testing, of which the ones\n",
    "we actually train and test on are a tiny sample. Thus, it is possible\n",
    "that observed differences in the reported performance are due to mere chance.\n",
    "\n",
    "There exist statistical methods which can be used to check for\n",
    "consistency (*statistical significance*) in the results, and one of the\n",
    "simplest such tests is the **sign test**. \n",
    "\n",
    "The sign test is based on the binomial distribution. Count all cases when System 1 is better than System 2, when System 2 is better than System 1, and when they are the same. Call these numbers $Plus$, $Minus$ and $Null$ respectively. \n",
    "\n",
    "The sign test returns the probability that the null hypothesis is true. \n",
    "\n",
    "This probability is called the $p$-value and it can be calculated for the two-sided sign test using the following formula (we multiply by two because this is a two-sided sign test and tests for the significance of differences in either direction):\n",
    "\n",
    "$$2 \\, \\sum\\limits_{i=0}^{k} \\binom{N}{i} \\, q^i \\, (1-q)^{N-i}$$\n",
    "\n",
    "where $$N = 2 \\Big\\lceil \\frac{Null}{2}\\Big\\rceil + Plus + Minus$$ is the total\n",
    "number of cases, and\n",
    "$$k = \\Big\\lceil \\frac{Null}{2}\\Big\\rceil + \\min\\{Plus,Minus\\}$$ is the number of\n",
    "cases with the less common sign. \n",
    "\n",
    "Here, we\n",
    "treat ties by adding half a point to either side, rounding up to the\n",
    "nearest integer if necessary. \n",
    "\n",
    "In this experiment, $q = 0.5$, so the formula simplifies to:\n",
    "$$2\\times 0.5^N\\, \\sum\\limits_{i=0}^{k} \\binom{N}{i}$$\n",
    "\n",
    "\n",
    "We use the `comb` function from `scipy` and the `decimal` package for the stable adding of numbers in the final summation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "de5l4oPkE-BS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.5024515744752503461039016873\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "from scipy.special import comb\n",
    "\n",
    "\n",
    "def sign_test(results_1, results_2):\n",
    "  \"\"\"test for significance\"\"\"\n",
    "  ties, plus, minus = 0, 0, 0\n",
    "\n",
    "  for i in range(0, len(results_1)):\n",
    "    if results_1[i]==results_2[i]:\n",
    "      ties += 1\n",
    "    elif results_1[i]==0: \n",
    "      plus += 1\n",
    "    elif results_2[i]==0: \n",
    "      minus += 1\n",
    "\n",
    "  n = (2 * math.ceil(ties/2.0)) + plus + minus\n",
    "  k = math.ceil(ties/2.0) + min(plus,minus)\n",
    "\n",
    "  summation = Decimal(0.0)\n",
    "  for i in range(0,int(k)+1):\n",
    "      summation += (Decimal(comb(n,i,exact=True)))\n",
    "\n",
    "  # use two-tailed version of test\n",
    "  summation *= 2\n",
    "  summation *= (Decimal(0.5)**Decimal(n))\n",
    "  \n",
    "  print(\"the difference is\", \n",
    "        \"not significant\" if summation >= 0.05 else \"significant\")\n",
    "  \n",
    "  return summation\n",
    "\n",
    "p_value = sign_test(token_results, magnitude_results)\n",
    "print(\"p_value =\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhU_tk-BOaXb"
   },
   "source": [
    "### Using the Sign test\n",
    "\n",
    "**From now on, report all differences between systems, as well as between system configurations, using the\n",
    "sign test.**\n",
    "    \n",
    "You should report statistical test\n",
    "results in an appropriate form. If there are several different methods\n",
    "(i.e., systems) to compare, the Sign test can only be applied to pairs of them\n",
    "at a time. When reporting these pair-wise differences, you should\n",
    "summarise trends to avoid redundancy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LibV4nR89BXb"
   },
   "source": [
    "# Naive Bayes (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnF9adQnuwia"
   },
   "source": [
    "\n",
    "Your second task is to program a simple Machine Learning approach that operates\n",
    "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
    "described by Pang et al. (2002). In this approach, the only features we\n",
    "will consider are the words in the text themselves, without bringing in\n",
    "external sources of information. The BoW model is a popular way of\n",
    "representing texts as vectors, making it\n",
    "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
    "However, the BoW representation is also very crude, since it discards\n",
    "all information related to word order and grammatical structure in the\n",
    "original text—as the name suggests.\n",
    "\n",
    "## Writing your own classifier (4pts)\n",
    "\n",
    "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
    "a reminder, the Naive Bayes classifier works according to the following\n",
    "equation:\n",
    "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
    "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
    "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
    "vector. Remember that we use the log of these probabilities when making\n",
    "a prediction:\n",
    "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
    "\n",
    "You can find more details about Naive Bayes in [Jurafsky &\n",
    "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
    "this helpful\n",
    "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
    "\n",
    "*Note: this section and the next aim to put you in a position to replicate\n",
    "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
    "    will differ from theirs, as they used different data.*\n",
    "\n",
    "**You must write the Naive Bayes training and prediction code from\n",
    "scratch.** You will not be given credit for using off-the-shelf Machine\n",
    "Learning libraries.\n",
    "\n",
    "The data contains the text of the reviews, where each document consists\n",
    "of the sentences in the review, the sentiment of the review and an index\n",
    "(cv) that you will later use for cross-validation. The\n",
    "text has already been tokenised and POS-tagged for you. Your algorithm\n",
    "should read in the text, **lowercase it**, store the words and their\n",
    "frequencies in an appropriate data structure that allows for easy\n",
    "computation of the probabilities used in the Naive Bayes algorithm, and\n",
    "then make predictions for new instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEpyQSBSkb33"
   },
   "source": [
    "#### (Q3.1) Unseen words (1pt)\n",
    "The presence of words in the test dataset that\n",
    "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
    "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes**. \n",
    "\n",
    "In case 2, you could also set $P(c|\\bar{f}) = 0$ for the class $c$ within which $f_i \\in \\bar{f}$ was not seen. One way to implement this in log space is to set $P(c|\\bar{f}) = -\\infty$. What would be the problem instead with skipping words only for one class in case 2? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BanFiYYnoxDW"
   },
   "source": [
    "#### answer \n",
    "If we do not skip both classes, a review with this word only seen in one class will always get probability 0 of belonging to the other class. This happens regardless of all the other words in the review. So we could have a review with all positive words and one word never seen in the positive class, this review would now get a probability 0 of being positive and (as long as all the other words are at least seen once in the negative class) will get a positive probability of being negative. \n",
    "Thats not very sensible. If we have multiple of such words, we could even have reviews that have probability 0 for both classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsZRhaI3WvzC"
   },
   "source": [
    "#### (Q3.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "G7zaJYGFvIJ3"
   },
   "outputs": [],
   "source": [
    "def create_dataset_split(reviews, cv_train, cv_test, remove_pos=[], remove_neg=[]):\n",
    "    '''\n",
    "        Creates a splitted dataset based on the reviews. \n",
    "    '''\n",
    "    \n",
    "    #### Split the reviews in the train/test sets\n",
    "    train_reviews = []\n",
    "    test_reviews = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        ### As long as we do not want to remove them we can add them:\n",
    "        if not ((review['cv'] in remove_pos and review['sentiment'] == \"POS\" )\n",
    "                 or (review['cv'] in remove_neg) and review['sentiment'] != \"POS\"):\n",
    "            if review['cv'] in cv_train:\n",
    "                train_reviews.append(review)\n",
    "            if review['cv'] in cv_test:\n",
    "                test_reviews.append(review)\n",
    "    ### Create and return the datasets\n",
    "    train_sentences, train_targets = create_dataset(train_reviews)\n",
    "    test_sentences, test_targets = create_dataset(test_reviews)\n",
    "    return train_sentences, train_targets, test_sentences, test_targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def create_voc(sentences):\n",
    "    '''\n",
    "        Creates a vocubulair of the sentences\n",
    "        \n",
    "        returns list of words.\n",
    "    '''\n",
    "    voc = list(set([word for sentence in sentences for word in sentence]))\n",
    "\n",
    "    return voc\n",
    "    \n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, k=0):\n",
    "        self.log_likelihood = defaultdict(float)\n",
    "        self.log_prior = defaultdict(float)\n",
    "        self.voc = None\n",
    "        self.counts = defaultdict(float)\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, sentences, targets):\n",
    "        '''\n",
    "            Fits the naive bayes classifier, given the sentences and the targets\n",
    "        '''\n",
    "        \n",
    "        ### Compute the relevant statistics.\n",
    "        n_doc = len(sentences)\n",
    "        n_pos = np.sum(targets)\n",
    "        n_neg = n_doc - n_pos\n",
    "        \n",
    "        self.log_prior = {\n",
    "            0: np.log(n_neg/n_doc),\n",
    "            1: np.log(n_pos/n_doc)\n",
    "        } \n",
    "        \n",
    "\n",
    "        ### Create the vocubulair.\n",
    "        self.voc = create_voc(sentences)\n",
    "        \n",
    "        ### Split in positive and negative sentences. \n",
    "        pos_idx = np.array(targets) == 1\n",
    "        doc = {\n",
    "            0: np.array(sentences, dtype=object)[~pos_idx],\n",
    "            1:  np.array(sentences, dtype=object)[pos_idx]\n",
    "        }\n",
    "        \n",
    "\n",
    "        ### Count the words for each class. \n",
    "        for c, sentences in doc.items():\n",
    "            for sentence in sentences:\n",
    "                for w in sentence:\n",
    "                    self.counts[(w,c)] += 1\n",
    "        \n",
    "        ### Keep track of the total counts, add the smoothing constant. \n",
    "        total_counts = {\n",
    "            0: np.sum([self.counts[(w,0)] + self.k if self.seen_by_both(w) else 0 for w in self.voc]),\n",
    "            1: np.sum([self.counts[(w,1)] + self.k if self.seen_by_both(w) else 0 for w in self.voc])\n",
    "        }\n",
    "\n",
    "        ### Calculate the log likelihood of each word class combination, add smoothing if neccasery. \n",
    "        for c in [0,1]:\n",
    "            if total_counts[c] != 0:\n",
    "                for w in self.voc:\n",
    "                    ### We only count words that have been seen by both the classes\n",
    "                    if self.seen_by_both(w):\n",
    "                        self.log_likelihood[(w, c)] = np.log( (self.counts[(w,c)] + self.k )/ total_counts[c])\n",
    "                        \n",
    "    def seen_by_both(self, w):\n",
    "        '''\n",
    "        checks if a word is seen by both:\n",
    "        '''\n",
    "        return self.counts[(w,0)] + self.k > 0 and self.counts[(w,1)] + self.k > 0\n",
    "    \n",
    "    def predict(self, sentences):\n",
    "        '''\n",
    "            Predict the label of the given sentences\n",
    "        '''\n",
    "        result = []\n",
    "        for sentence in tqdm(sentences):\n",
    "            result.append(self.predict_1(sentence))\n",
    "        return result\n",
    "    \n",
    "    def predict_1(self, sentence):\n",
    "        '''\n",
    "            Predicts the label of one sentence\n",
    "        '''\n",
    "        \n",
    "        sum_c = [self.log_prior[0], self.log_prior[1]]\n",
    "        for c in [0,1]:\n",
    "            for word in sentence:\n",
    "                    ### Note that we implicitally use that words that did not appear do not count, by using a default dict\n",
    "                    sum_c[c] += self.log_likelihood[(word, c)]\n",
    "        return np.argmax(sum_c)\n",
    "\n",
    "    def reset_params(self):\n",
    "        '''\n",
    "        Resets the parameters, such that we can do another round of trainig.\n",
    "        '''\n",
    "        self.log_likelihood = defaultdict(float)\n",
    "        self.log_prior = defaultdict(float)\n",
    "        self.voc = None\n",
    "        self.counts = defaultdict(float)     \n",
    "    \n",
    "train_sentences, train_targets, test_sentences, test_targets = create_dataset_split(reviews, [i for i in range(0,900)], [i for i in range(900,1000)])\n",
    "naive_bayes_model = NaiveBayesClassifier()\n",
    "naive_bayes_model.fit(train_sentences, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2000.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = naive_bayes_model.predict(test_sentences)\n",
    "\n",
    "naive_bayes_results = calculate_results(predictions, test_targets)\n",
    "naive_bayes_accuracy = calculate_accuracy(predictions, test_targets)\n",
    "print(\"Accuracy: %0.3f\" % naive_bayes_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0INK-PBoM6CB"
   },
   "source": [
    "#### (Q3.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
    "\n",
    "Simulate this scenario by keeping the positive reviews\n",
    "data unchanged, but only using negative reviews cv000–cv089 for\n",
    "training, and cv900–cv909 for testing. Calculate the classification\n",
    "accuracy, and explain what changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFbcsYlipBAw"
   },
   "source": [
    "No, accuracy would no longer be a good measure. If 90% of the data is positive, we would get a 90% accuracy with a model that always outputs positive. However, this is clearly not a good model. We should use a different metric for such cases.\n",
    "\n",
    "In our case, we would expect the log priors to be skewed towards positive data as this is now 90% of the data. This should result in more cases being classified at positive and thus in a higher accuracy, as our test data is mostly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GWDkt5ZrrFGp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 2055.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_targets, test_sentences, test_targets = create_dataset_split(reviews, [i for i in range(0,900)], [i for i in range(900,1000)], [], [i for i in range(90, 900)] \n",
    "                                                                                    + [i for i in range(910, 1000)])\n",
    "\n",
    "skewed_naive_bayes = NaiveBayesClassifier()\n",
    "skewed_naive_bayes.fit(train_sentences, train_targets)\n",
    "predictions = skewed_naive_bayes.predict(test_sentences)\n",
    "naive_bayes_all_neg_results = calculate_results(predictions , test_targets)\n",
    "\n",
    "naive_bayes_all_neg_accuracy = calculate_accuracy(predictions, test_targets)\n",
    "print(\"Accuracy: %0.2f\" % naive_bayes_all_neg_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy becomes close to 0.9, which is what we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wJzcHX3WUDm"
   },
   "source": [
    "## Smoothing (1.5pts)\n",
    "\n",
    "As mentioned above, the presence of words in the test dataset that\n",
    "have not been seen during training can cause probabilities in the Naive\n",
    "Bayes classifier to be $0$, thus making that particular test instance\n",
    "undecidable. The standard way to mitigate this effect (as well as to\n",
    "give more clout to rare words) is to use smoothing, in which the\n",
    "probability fraction\n",
    "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ for a word\n",
    "$w_i$ becomes\n",
    "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBNIcbwUWphC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Q3.4) Implement Laplace feature smoothing (1pt)\n",
    "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
    "Bayes classifier’s code, and report the impact on performance. \n",
    "Use $\\kappa = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "g03yflCc9kpW"
   },
   "outputs": [],
   "source": [
    "train_sentences, train_targets, test_sentences, test_targets = create_dataset_split(reviews, [i for i in range(0,900)], [i for i in range(900,1000)])\n",
    "smooth_naive_bayes_model = NaiveBayesClassifier(1)\n",
    "smooth_naive_bayes_model.fit(train_sentences, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1923.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = smooth_naive_bayes_model.predict(test_sentences)\n",
    "\n",
    "smoothed_naive_bayes_results = calculate_results(predictions, test_targets)\n",
    "smoothed_naive_bayes_accuracy = calculate_accuracy(predictions, test_targets)\n",
    "print(\"Accuracy: %0.3f\" % smoothed_naive_bayes_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-conSBddWWyN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Q3.5) Is the difference between non smoothed (Q3.2) and smoothed (Q3.4) statistically significant? (0.5pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "CCvSNGlHMUPz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.9436515209907435777527547357\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_value = sign_test(smoothed_naive_bayes_results, naive_bayes_results)\n",
    "print(\"p_value =\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the difference between non smoothed and smoothed variant is not statistically significant. So there is not a statistically significant impact on performance. \n",
    "\n",
    "This is probably due the fact that our naive bayes skips unseen words, with has a similar effect as smoothing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiGcgwba87D5"
   },
   "source": [
    "## Cross-Validation (1.5pts)\n",
    "\n",
    "A serious danger in using Machine Learning on small datasets, with many\n",
    "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
    "suggested by the data” errors. This type of error occurs when we make\n",
    "repeated improvements to our classifiers by playing with features and\n",
    "their processing, but we don’t get a fresh, never-before seen test\n",
    "dataset every time. Thus, we risk developing a classifier that gets better\n",
    "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
    "\n",
    "A simple method to guard against Type III errors is to use\n",
    "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
    "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
    "time holding out one of the folds for testing, training our classifier\n",
    "on the remaining N - 1 data folds, and reporting performance on the\n",
    "held-out fold. We can use different strategies for dividing the data:\n",
    "\n",
    "-   Consecutive splitting:\n",
    "  - cv000–cv099 = Split 1\n",
    "  - cv100–cv199 = Split 2\n",
    "  - etc.\n",
    "  \n",
    "-   Round-robin splitting (mod 10):\n",
    "  - cv000, cv010, cv020, … = Split 1\n",
    "  - cv001, cv011, cv021, … = Split 2\n",
    "  - etc.\n",
    "\n",
    "-   Random sampling/splitting\n",
    "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OeLcbSauGtR"
   },
   "source": [
    "#### (Q3.6) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q3.2 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3KeCGPa7Nuzx"
   },
   "outputs": [],
   "source": [
    "def create_folds(data, n_folds):\n",
    "    '''\n",
    "    Creates folds using round-robin splitting\n",
    "    '''\n",
    "    review_folds = [[] for i in range(n_folds)]\n",
    "    label_folds = [[] for i in range(n_folds)]\n",
    "\n",
    "    for review in data:\n",
    "        i = review['cv'] % n_folds\n",
    "        whole_review = content_to_sentence( review['content'])\n",
    "        review_folds[i].append(whole_review)\n",
    "        label_folds[i].append(int( review['sentiment'] == 'POS'))\n",
    "    return review_folds, label_folds\n",
    "\n",
    "        \n",
    "review_folds, label_folds = create_folds(reviews, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_fold(model, x_folds, y_folds, held_out_index):\n",
    "    '''\n",
    "        Train the given model on the folds. Use held_out_index to check what the held out fold is.\n",
    "    '''\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for i in range(len(x_folds)):\n",
    "        if i != held_out_index:\n",
    "            train_x += x_folds[i]\n",
    "            train_y += y_folds[i]\n",
    "        else:\n",
    "            test_x += x_folds[i]\n",
    "            test_y += y_folds[i]\n",
    "\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    predictions = model.predict(test_x)\n",
    "    results = calculate_results(predictions, test_y)\n",
    "    accuracy  = calculate_accuracy(predictions, test_y)\n",
    "    return accuracy, predictions, results\n",
    "\n",
    "def train_on_folds(create_model, x_folds, y_folds):\n",
    "    '''\n",
    "        Trains a model on the given folds.\n",
    "        Create_model: a function that returns a model that we want to train\n",
    "        x_folds: list of folds with the input features\n",
    "        y_folds: list of folds with the target class. \n",
    "    '''\n",
    "    models = [create_model() for i in range(len(x_folds))]\n",
    "    accuracies = []\n",
    "    predictions = []\n",
    "    results = []\n",
    "    \n",
    "    ### Loop over the folds. \n",
    "    for i, fold in enumerate(x_folds):\n",
    "        model = models[i]\n",
    "        accuracy, prediction, result = train_on_fold(model, x_folds, y_folds, i)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        predictions.append(predictions)\n",
    "        results.append(result)\n",
    "    return models, accuracies, predictions, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1834.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1922.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1934.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1869.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2019.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1869.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1835.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1885.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1904.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2027.68it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Function to create a naive bayes model with k=1\n",
    "create_naive_bayes_model = lambda: NaiveBayesClassifier(1)\n",
    "naive_bayes_models, naive_bayes_accuracies, naive_bayes_predictions, naive_bayes_results = train_on_folds(create_naive_bayes_model, review_folds, label_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:  [0.79, 0.835, 0.805, 0.825, 0.78, 0.845, 0.83, 0.775, 0.83, 0.84]\n",
      "mean:  0.8154999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracies: \", naive_bayes_accuracies)\n",
    "print(\"mean: \", np.mean(naive_bayes_accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otdlsDXBNyOa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Q3.7) Write code to calculate and report the variance of the 10 accuracy scores, in addition to the final performance. You must not use a library, such as numpy. (0.5pt)\n",
    "\n",
    "**Please report all future results using 10-fold cross-validation now\n",
    "(unless told to use the held-out test set).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ZoBQm1KuNzNR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79, 0.835, 0.805, 0.825, 0.78, 0.845, 0.83, 0.775, 0.83, 0.84]\n",
      "0.8155000000000001\n",
      "0.024540782383616025\n"
     ]
    }
   ],
   "source": [
    "def compute_mean(values):\n",
    "    '''\n",
    "        Computes the mean of the given values\n",
    "    '''\n",
    "    total = 0\n",
    "    for val in values:\n",
    "        total += val\n",
    "    return total/len(values)\n",
    "\n",
    "def compute_std(values):\n",
    "    '''\n",
    "        Computes the standard deviation of the given values\n",
    "    '''\n",
    "    mean = compute_mean(values)\n",
    "    tmp_sum = 0\n",
    "    for val in values:\n",
    "        tmp_sum += (val- mean)**2\n",
    "    return math.sqrt(1/len(values) * tmp_sum )\n",
    "\n",
    "\n",
    "print(naive_bayes_accuracies)\n",
    "print(compute_mean(naive_bayes_accuracies))\n",
    "print(compute_std(naive_bayes_accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6A2zX9_BRKm"
   },
   "source": [
    "## Features, overfitting, and the curse of dimensionality\n",
    "\n",
    "In the Bag-of-Words model, ideally we would like each distinct word in\n",
    "the text to be mapped to its own dimension in the output vector\n",
    "representation. However, real world text is messy, and we need to decide\n",
    "on what we consider to be a word. For example, is “`word`\" different\n",
    "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
    "definition, and the number of features explodes, while our algorithm\n",
    "fails to learn anything generalisable. Too lax, and we risk destroying\n",
    "our learning signal. In the following section, you will learn about\n",
    "confronting the feature sparsity and the overfitting problems as they\n",
    "occur in NLP classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKK8FNt8VtcZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stemming (1.5pts)\n",
    "\n",
    "To make your algorithm more robust, use stemming and\n",
    "hash different inflections of a word to the same feature in the BoW\n",
    "vector space. How does the performance of your classifier change when\n",
    "you use stemming on your training and test datasets? Please use the [Porter stemming\n",
    "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
    "\n",
    "You should also perform cross-validation. Concatenate the predictions from all folds to compute the significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "NxtCul1IrBi_"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import *\n",
    "\n",
    "def stem_sentences(sentences):\n",
    "    \n",
    "    '''\n",
    "        Applies porter stemmer to all words in the sentences using a look-up table\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_dict = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word not in stem_dict:\n",
    "                stem_dict[word] = stemmer.stem(word)\n",
    "            sentences[i][j] = stem_dict[word]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "### Preprocess the sentences\n",
    "stemmed_x_folds = [stem_sentences(x) for x in review_folds]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2082.98it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2020.23it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1802.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2062.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2061.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2173.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1818.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2127.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1818.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2105.30it/s]\n"
     ]
    }
   ],
   "source": [
    "stemmed_models, stemmed_accuracies, stemmed_predictions, stemmed_results  = train_on_folds(create_naive_bayes_model, stemmed_x_folds, label_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78, 0.84, 0.805, 0.84, 0.775, 0.84, 0.82, 0.775, 0.83, 0.83]\n",
      "0.8135\n",
      "0.026177280225416825\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_accuracies)\n",
    "print(compute_mean(stemmed_accuracies))\n",
    "print(compute_std(stemmed_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SrJ1BeLXTnk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Q3.9): Is the difference between NB with smoothing and NB with smoothing+stemming significant? (1pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "gYqKBOiIrInT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.9465186089423488346539016590\n"
     ]
    }
   ],
   "source": [
    "## Concatonate the results in one array\n",
    "concat_stemmed_results = np.array(stemmed_results).reshape(-1)\n",
    "concat_naive_bayes_results = np.array(naive_bayes_results).reshape(-1)\n",
    "### \n",
    "p_value = sign_test(concat_stemmed_results, concat_naive_bayes_results)\n",
    "print(\"p_value =\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkDHVq_1XUVP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Q3.10) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q3.4)? (0.5pt)\n",
    "Give actual numbers. You can use the held-out training set to determine these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "MA3vee5-rJyy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unstemmed features: 45348\n",
      "Stemmed features: 32404\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_targets, test_sentences, test_targets = create_dataset_split(reviews, [i for i in range(0,900)], [i for i in range(900,1000)])\n",
    "\n",
    "unstemmed_model = NaiveBayesClassifier(k=1)\n",
    "unstemmed_model.fit(train_sentences, train_targets)\n",
    "print(\"Unstemmed features: {}\".format(len(unstemmed_model.voc)))\n",
    "\n",
    "stemmed_model = NaiveBayesClassifier(k=1)\n",
    "stemmed_train_sentences = stem_sentences(train_sentences)\n",
    "stemmed_model.fit(stemmed_train_sentences, train_targets)\n",
    "print(\"Stemmed features: {}\".format(len(stemmed_model.voc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So the number of features has decreased by 12944\n"
     ]
    }
   ],
   "source": [
    "print(\"So the number of features has decreased by {}\".format(len(unstemmed_model.voc) - len(stemmed_model.voc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoazfxbNV5Lq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### N-grams (1.5pts)\n",
    "\n",
    "A simple way of retaining some of the word\n",
    "order information when using bag-of-words representations is to add **n-gram** features. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHjy3I7-qWiu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Q3.11) Retrain your classifier from (Q3.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
    "Report accuracy and statistical significances (in comparison to the experiment at (Q3.6) for all 10 folds, and between the new systems). You are allowed to use NLTK to build n-grams from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "eYuKMTOpq9jz"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "def to_n_grams(sentences, ns=[1,2,3]):\n",
    "    '''\n",
    "        Creates n grams from the sentences\n",
    "        sentences: the sentences to create n_grams from\n",
    "        ns: list of grams that we want to create\n",
    "        returns: list containing sentences of ngrams. \n",
    "    '''\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        temp = []\n",
    "        for n in ns:\n",
    "            temp += ngrams(sentence, n)\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "\n",
    "un_bi_gram_x = [to_n_grams(x, ns=[1,2]) for x in review_folds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 582.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 527.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 520.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 548.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 564.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 579.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 539.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 578.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 575.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 595.29it/s]\n"
     ]
    }
   ],
   "source": [
    "un_bi_models, un_bi_accuracies, un_bi_predictions, un_bi_results  = train_on_folds(create_naive_bayes_model, un_bi_gram_x, label_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram and bigram accuracies:  [0.79, 0.845, 0.835, 0.87, 0.805, 0.855, 0.845, 0.815, 0.84, 0.83]\n",
      "mean: 0.833 and std: 0.02271563338320108\n"
     ]
    }
   ],
   "source": [
    "print(\"unigram and bigram accuracies: \", un_bi_accuracies)\n",
    "print(\"mean: {} and std: {}\".format(compute_mean(un_bi_accuracies), compute_std(un_bi_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.4472188836140076235320103833\n"
     ]
    }
   ],
   "source": [
    "## Concatonate the results in one array\n",
    "concat_un_bi_results = np.array(un_bi_results).reshape(-1)\n",
    "concat_naive_bayes_results = np.array(naive_bayes_results).reshape(-1)\n",
    "\n",
    "\n",
    "### \n",
    "p_value = sign_test(concat_un_bi_results, concat_naive_bayes_results)\n",
    "print(\"p_value =\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 300.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 289.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 289.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 296.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 311.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 297.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 280.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 302.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 290.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 299.37it/s]\n"
     ]
    }
   ],
   "source": [
    "un_bi_tri_gram_x = [to_n_grams(x, ns=[1,2, 3]) for x in review_folds]\n",
    "un_bi_tri_models, un_bi_tri_accuracies, un_bi_tri_predictions, un_bi_tri_results  = train_on_folds(create_naive_bayes_model, un_bi_tri_gram_x, label_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram, bigram  and trigram accuracies:  [0.785, 0.825, 0.83, 0.855, 0.815, 0.845, 0.85, 0.83, 0.84, 0.835]\n",
      "mean: 0.8309999999999998 and std: 0.019078784028338902\n"
     ]
    }
   ],
   "source": [
    "print(\"unigram, bigram  and trigram accuracies: \", un_bi_tri_accuracies)\n",
    "print(\"mean: {} and std: {}\".format(compute_mean(un_bi_tri_accuracies), compute_std(un_bi_tri_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.5024515744752503461039016873\n"
     ]
    }
   ],
   "source": [
    "## Concatonate the results in one array\n",
    "concat_un_bi_tri_results = np.array(un_bi_tri_results).reshape(-1)\n",
    "concat_naive_bayes_results = np.array(naive_bayes_results).reshape(-1)\n",
    "\n",
    "\n",
    "### \n",
    "p_value = sign_test(concat_un_bi_tri_results, concat_naive_bayes_results)\n",
    "print(\"p_value =\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVrGGArkrWoL"
   },
   "source": [
    "\n",
    "#### Q3.12: How many features does the BoW model have to take into account now? (0.5pt)\n",
    "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q3.10)?\n",
    "\n",
    "Use the held-out training set once again for this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEGZ9SV8pPaa"
   },
   "source": [
    "Theoretically I would expect that the number of features for O(m^n) where m is the number of unigrams and n is the highest ngram that we use. Because when we use n-grams with m words there are m^n possible different n_grams.\n",
    "A lot of the n_grams do not appear in practice hence we would expect to have, in practice, a lot less feautures.\n",
    "As you can see from the prints below, a tiny fraction of the actual number of features is shown in the model that uses un, bi and tri-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "_z8sAJeUrdtM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_gram features: 45348\n",
      "expected 1 and 2 gram features: 2056441104\n",
      "observed 1 and 2 gram features: 471032\n",
      "difference 2055970072\n",
      "expected 1, 2 and 3 gram features: 93255491184192\n",
      "observed 1, 2  and 3 gram features: 1416686\n",
      "difference 93255489767506\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_targets, test_sentences, test_targets = create_dataset_split(reviews, [i for i in range(0,900)], [i for i in range(900,1000)])\n",
    "\n",
    "un_bi_gram_sentences = to_n_grams(train_sentences, [1, 2])\n",
    "un_bi_tri_gram_sentences = to_n_grams(train_sentences, [1,2, 3])\n",
    "\n",
    "\n",
    "un_model = NaiveBayesClassifier()\n",
    "un_model.fit(train_sentences, train_targets)\n",
    "\n",
    "\n",
    "un_bi_model = NaiveBayesClassifier()\n",
    "un_bi_model.fit(un_bi_gram_sentences, train_targets)\n",
    "\n",
    "un_bi_tri_model = NaiveBayesClassifier()\n",
    "un_bi_tri_model.fit(un_bi_tri_gram_sentences, train_targets)\n",
    "\n",
    "\n",
    "n_un_gram_features = len(un_model.voc)\n",
    "n_un_bi_gram_features = len(un_bi_model.voc)\n",
    "n_un_bi_tri_gram_features = len(un_bi_tri_model.voc)\n",
    "\n",
    "print(\"1_gram features: {}\".format(n_un_gram_features))\n",
    "print(\"expected 1 and 2 gram features: {}\".format(n_un_gram_features ** 2))\n",
    "print(\"observed 1 and 2 gram features: {}\".format(n_un_bi_gram_features))\n",
    "print(\"difference {}\".format(n_un_gram_features**2 - n_un_bi_gram_features ))\n",
    "\n",
    "print(\"expected 1, 2 and 3 gram features: {}\".format(n_un_gram_features ** 3))\n",
    "print(\"observed 1, 2  and 3 gram features: {}\".format(n_un_bi_tri_gram_features))\n",
    "print(\"difference {}\".format(n_un_gram_features**3 - n_un_bi_tri_gram_features ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHWKDL3YV6vh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Support Vector Machines (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJSYhcVaoJGt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Though simple to understand, implement, and debug, one\n",
    "major problem with the Naive Bayes classifier is that its performance\n",
    "deteriorates (becomes skewed) when it is being used with features which\n",
    "are not independent (i.e., are correlated). Another popular classifier\n",
    "that doesn’t scale as well to big data, and is not as simple to debug as\n",
    "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
    "Vector Machine (SVM) classifier.\n",
    "\n",
    "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
    "Other sources for learning SVM:\n",
    "* http://web.mit.edu/zoya/www/SVM.pdf\n",
    "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
    "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Use the scikit-learn implementation of \n",
    "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LnzNtQBV8gr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Q4.1): Train SVM and compare to Naive Bayes (2pts)\n",
    "\n",
    "Train an SVM classifier (sklearn.svm.LinearSVC) using your features. Compare the\n",
    "classification performance of the SVM classifier to that of the Naive\n",
    "Bayes classifier with smoothing from (Q3.6) and report the numbers.\n",
    "Perform cross-validation and concatenate the predictions from all folds to compute the significance.  Are the results significantly better?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell contains a lot of code that we will reuse.\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csc_matrix\n",
    "def create_fold_indexes(reviews, n_folds):\n",
    "    '''\n",
    "    Creates folds using round-robin splitting. \n",
    "    Returns the index of the folds in which each review should be\n",
    "    '''\n",
    "    indexes = []\n",
    "\n",
    "    for (index, review) in enumerate(reviews):\n",
    "        i = review['cv'] % n_folds\n",
    "        indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "def create_voc_of_features(X):\n",
    "    '''\n",
    "    Creates a vocabulary of the features that we encounter in the features\n",
    "    :param X: List features for each datapoint\n",
    "    :return: a vocabulary of the features\n",
    "    '''\n",
    "    voc = list(set([feature for features in X for feature in features]))\n",
    "\n",
    "    return voc\n",
    "\n",
    "\n",
    "def create_index_mapping(voc):\n",
    "    '''\n",
    "    Creates an index representation of the vocabulary\n",
    "    First item will have index 1, second 2 etc.. Will reserve index 0 for unseen features.\n",
    "    :param voc: The vocabulary to create index representation\n",
    "    :return: a defaultdict containing a mapping from word to index\n",
    "    '''\n",
    "    one_hot_mapping = defaultdict(float)\n",
    "    for i in tqdm(range(1, len(voc) + 1)):\n",
    "        word = voc[i - 1]\n",
    "        one_hot_mapping[word] = i\n",
    "    return one_hot_mapping\n",
    "\n",
    "def prepare_dataset(reviews, feature_function):\n",
    "    '''\n",
    "    Prepares the dataset, in which each review get prepared using the feature_function\n",
    "    :param reviews:\n",
    "    :param feature_map:\n",
    "    :return:\n",
    "    '''\n",
    "    targets = []\n",
    "    X = []\n",
    "\n",
    "    ### Loop over the reviews, get the target, and make 1 sentence of the content.\n",
    "    for review in reviews:\n",
    "        targets.append(int(review['sentiment'] == 'POS'))\n",
    "\n",
    "        whole_review = feature_function(review)\n",
    "        X.append(whole_review)\n",
    "    return X, targets\n",
    "\n",
    "def review_to_sentence(review):\n",
    "    '''\n",
    "        Creates a sentence (list) of the content of a review\n",
    "        content: [(word: string, POS: string)]\n",
    "        returns: [word: string]\n",
    "    '''\n",
    "    whole_review = []\n",
    "    for sentence in review['content']:\n",
    "        whole_review += [word.lower() for word in np.array(sentence)[:, 0]]\n",
    "\n",
    "    return whole_review\n",
    "\n",
    "def review_to_words_pos(review):\n",
    "    '''\n",
    "        Creates a sentence (list) of the content of a review\n",
    "        content: [(word: string, POS: string)]\n",
    "        returns: [word: string]\n",
    "    '''\n",
    "    result = []\n",
    "    for sentence in review['content']:\n",
    "        result += [(word.lower(), pos) for word, pos in np.array(sentence)]\n",
    "\n",
    "    return result\n",
    "\n",
    "def review_to_words_pos_filtered(review):\n",
    "    '''\n",
    "        Creates a sentence (list) of the content of a review\n",
    "        content: [(word: string, POS: string)]\n",
    "        returns: [word: string]\n",
    "    '''\n",
    "    to_keep =  [\n",
    "        'NN',\n",
    "        'VB',\n",
    "        'JJ',\n",
    "        'RB',\n",
    "    ]\n",
    "    result = []\n",
    "    for sentence in review['content']:\n",
    "        result += [(word.lower(), pos) for word, pos in np.array(sentence) if pos in to_keep]\n",
    "\n",
    "    return result\n",
    "\n",
    "def features_to_indexes(features, index_mapping):\n",
    "    '''\n",
    "    Maps the features to their index\n",
    "    :param features:\n",
    "    :param index_mapping:\n",
    "    :return:\n",
    "    '''\n",
    "    return [index_mapping[feature] for feature in features]\n",
    "\n",
    "\n",
    "def counter_to_features(counter, n_features):\n",
    "    '''\n",
    "    Creates a vector containing the count of each index\n",
    "    (Remember that each index represents a feature)\n",
    "    :param counter: a Counter containing indexes and there counts\n",
    "    :param n_features: the number of features that are used.\n",
    "    :return: A numpy array containing the count of each feature in the corresponing index.\n",
    "    '''\n",
    "    result = np.zeros(n_features)\n",
    "    for key, val in counter.items():\n",
    "        result[key] = val\n",
    "    return result\n",
    "\n",
    "def create_feature_matrix(X, index_mapping):\n",
    "    '''\n",
    "    Create a sparse representation of the features using the given index map\n",
    "    :param X:\n",
    "    :param index_mapping:\n",
    "    :return:\n",
    "    '''\n",
    "    result = []\n",
    "    n_features = len(index_mapping) + 1\n",
    "    for i in tqdm(range(len(X))):\n",
    "        result.append(\n",
    "            counter_to_features(\n",
    "                Counter(features_to_indexes(X[i], index_mapping)),\n",
    "                n_features\n",
    "            )\n",
    "        )\n",
    "    return result\n",
    "\n",
    "\n",
    "def train_on_fold_svm(model, x_folds, y_folds, held_out_index):\n",
    "    '''\n",
    "        Train the given model on the folds. Use held_out_index to check what the held out fold is.\n",
    "    '''\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for i in range(len(x_folds)):\n",
    "        if i != held_out_index:\n",
    "            train_x += x_folds[i]\n",
    "            train_y += y_folds[i]\n",
    "        else:\n",
    "            test_x = x_folds[i]\n",
    "            test_y += y_folds[i]\n",
    "\n",
    "    train_x = csc_matrix(train_x)\n",
    "    test_x = csc_matrix(test_x)\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    predictions = model.predict(test_x)\n",
    "    results = calculate_results(predictions, test_y)\n",
    "    accuracy = calculate_accuracy(predictions, test_y)\n",
    "    return accuracy, predictions, results\n",
    "\n",
    "\n",
    "def train_on_folds_svm(create_model, X, Y, fold_indexes):\n",
    "    '''\n",
    "        Trains a model on the given folds.\n",
    "        Create_model: a function that returns a model that we want to train\n",
    "        x_folds: list of folds with the input features\n",
    "        y_folds: list of folds with the target class.\n",
    "    '''\n",
    "\n",
    "    ### Create the actual folds.\n",
    "    x_folds = defaultdict(list)\n",
    "    y_folds = defaultdict(list)\n",
    "    for index, x,y  in zip(fold_indexes, X, Y):\n",
    "        x_folds[index] += [x]\n",
    "        y_folds[index] += [y]\n",
    "    \n",
    "    models = [create_model() for i in range(len(x_folds))]\n",
    "\n",
    "    accuracies = []\n",
    "    predictions = []\n",
    "    results = []\n",
    "\n",
    "    ### Loop over the folds.\n",
    "    for i in tqdm(range(len(x_folds))):\n",
    "\n",
    "        model = models[i]\n",
    "        accuracy, prediction, result = train_on_fold_svm(model, x_folds, y_folds, i)\n",
    "        accuracies.append(accuracy)\n",
    "        predictions.append(predictions)\n",
    "        results.append(result)\n",
    "    result_dict = {\n",
    "        \"models\": models,\n",
    "        'accuracies': accuracies,\n",
    "        'predictions': predictions,\n",
    "        'results': results\n",
    "    }\n",
    "    return result_dict\n",
    "\n",
    "def compare_model_results(results_model_1, results_model_2):\n",
    "    ## Concatonate the results in one array\n",
    "    concat_results_model_1 = np.array(results_model_1).reshape(-1)\n",
    "    concat_results_model_2 = np.array(results_model_2).reshape(-1)\n",
    "    \n",
    "    p_value = sign_test(concat_results_model_1, concat_results_model_2)\n",
    "    print(\"p_value =\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "JBscui8Mvoz0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47743/47743 [00:00<00:00, 2076062.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1321.00it/s]\n",
      " 10%|████████████████████                                                                                                                                                                                    | 1/10 [00:02<00:21,  2.44s/it]C:\\Users\\gerso\\anaconda3\\envs\\nlp1\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:15<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81, 0.795, 0.8, 0.84, 0.85, 0.815, 0.845, 0.85, 0.875, 0.84]\n",
      "0.8320000000000001\n",
      "0.024413111231467388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Now we will create our svm classifier and do cross validation\n",
    "X, Y = prepare_dataset(reviews, review_to_sentence)\n",
    "\n",
    "\n",
    "voc = create_voc_of_features(X)\n",
    "\n",
    "index_mapping = create_index_mapping(voc)\n",
    "\n",
    "X = create_feature_matrix(X, index_mapping)\n",
    "\n",
    "\n",
    "fold_indexes = create_fold_indexes(reviews, n_folds=10)\n",
    "\n",
    "\n",
    "word_result_dict = train_on_folds_svm(svm.LinearSVC, X,Y, fold_indexes)\n",
    "\n",
    "print(word_result_dict['accuracies'])\n",
    "print(compute_mean(word_result_dict['accuracies']))\n",
    "print(compute_std(word_result_dict['accuracies']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.4743938089378458261960028581\n"
     ]
    }
   ],
   "source": [
    "compare_model_results(word_result_dict['results'], naive_bayes_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM is not significant better or worse than the naive bayes model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifXVWcK0V9qY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### POS disambiguation (2pts)\n",
    "\n",
    "Now add in part-of-speech features. You will find the\n",
    "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
    "replicate the results obtained by Pang et al. (2002).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA3I82o4oWGu"
   },
   "source": [
    "####(Q4.2) Replace your features with word+POS features, and report performance with the SVM. Does this help? Perform cross-validation and concatenate the predictions from all folds to compute the significance. Are the results significant? *Why?*  (1pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57537/57537 [00:00<00:00, 1742048.13it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 2365.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:16<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82, 0.795, 0.825, 0.84, 0.84, 0.845, 0.855, 0.855, 0.865, 0.84]\n",
      "0.8380000000000001\n",
      "0.019261360284258216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y = prepare_dataset(reviews, review_to_words_pos)\n",
    "\n",
    "\n",
    "voc = create_voc_of_features(X)\n",
    "\n",
    "index_mapping = create_index_mapping(voc)\n",
    "\n",
    "X = create_feature_matrix(X, index_mapping)\n",
    "\n",
    "\n",
    "fold_indexes = create_fold_indexes(reviews, n_folds=10)\n",
    "\n",
    "\n",
    "word_pos_result_dict = train_on_folds_svm(svm.LinearSVC, X,Y, fold_indexes)\n",
    "\n",
    "\n",
    "print(word_pos_result_dict['accuracies'])\n",
    "print(compute_mean(word_pos_result_dict['accuracies']))\n",
    "print(compute_std(word_pos_result_dict['accuracies']))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.3253010708560259189094236949\n"
     ]
    }
   ],
   "source": [
    "compare_model_results(word_pos_result_dict['results'], naive_bayes_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.8057148676803825624105038557\n"
     ]
    }
   ],
   "source": [
    "compare_model_results(word_pos_result_dict['results'], word_result_dict['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the svm with word and pos features is not significant better or worse than the naive bayes model or the svm with only word features.\n",
    "This may be due the fact that a tag does not provide (much) aditional information about the sentiment of a word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Q4.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Perform cross-validation and concatenate the predictions from all folds to compute the significance. Are the results significantly better than when we don't discard the closed-class words? *Why?* (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28167/28167 [00:00<00:00, 2012862.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 5681.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84, 0.82, 0.805, 0.865, 0.84, 0.79, 0.865, 0.86, 0.815, 0.81]\n",
      "0.8310000000000001\n",
      "0.02547547840571398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y = prepare_dataset(reviews, review_to_words_pos_filtered)\n",
    "\n",
    "voc = create_voc_of_features(X)\n",
    "\n",
    "index_mapping = create_index_mapping(voc)\n",
    "\n",
    "X = create_feature_matrix(X, index_mapping)\n",
    "\n",
    "fold_indexes = create_fold_indexes(reviews, n_folds=10)\n",
    "\n",
    "word_pos_discarded_result_dict = train_on_folds_svm(svm.LinearSVC, X, Y, fold_indexes)\n",
    "\n",
    "print(word_pos_discarded_result_dict['accuracies'])\n",
    "print(compute_mean(word_pos_discarded_result_dict['accuracies']))\n",
    "print(compute_std(word_pos_discarded_result_dict['accuracies']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.9821609888541456792697619327\n"
     ]
    }
   ],
   "source": [
    "compare_model_results(word_pos_discarded_result_dict['results'], word_result_dict['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.7712977979137471160722421800\n"
     ]
    }
   ],
   "source": [
    "compare_model_results(word_pos_discarded_result_dict['results'], word_pos_result_dict['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is not significant\n",
      "p_value = 0.5024515744752503461039016873\n"
     ]
    }
   ],
   "source": [
    "compare_model_results(word_pos_discarded_result_dict['results'], naive_bayes_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaxCVrs8pWSp"
   },
   "source": [
    "The last model is also not significant better or worse than the other models. \n",
    "Keeping only the clossed-class words does not improve the model. But it also does not make the model significant worse. This is probably because the clossed-class words contain the most information about the sentiment of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfwqOciAl2No"
   },
   "source": [
    "# (Q5) Discussion (max. 500 words). (5pts)\n",
    "\n",
    "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
    "Why is this important? What are the limitations of these features and techniques?\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYuse5WLmekZ"
   },
   "source": [
    "First of all, it seems that just counting the positive and negative words in a review gives a good baseline in sentiment analysis. Using more sophisticated statistical methods that use of bag of words significantly improve upon those baselines.\n",
    "\n",
    "First we see that naive bayes improves upon this baseline, but it does not matter whether unigrams, bigrams or trigrams are used. It seems that unigrams convey enough sentimental information to make good predictions. Ngrams encode (limited) information about word order and thus bigrams and trigrams could make sense of negations (e.g. not good vs not bad). Making sense of the negations could in theory help but in practice this does not seem to make a significant difference. \n",
    "Furthermore smoothing does not help when the models skips unseen words, but smoothing is probably more reliable because it does not throw out features that it did not see before. One of both is needed to make the model perform better than the baseline, because otherwise the model can not deal well with unseen words. \n",
    "\n",
    "When using stemming we make sure that different forms of a word are mapped to the same feature, but this does not seem to make a difference. Stemming does however create a big reduction in the number of features that are present, this helps with dimensionality reduction of the feature space.\n",
    "\n",
    "When we use SVM instead of naive bayes we do not get a significant different result. This would indicate that the naive bayes assumption is a reasonable assumption when working with bag of words. \n",
    "\n",
    "POS tags encode information about the word type. When using word with POS tags we do not see an significant improvement in performance, this indicates that POS tags do not convey much information about the sentiment of a review. For example knowing that a word is an adjective does not tell us much about the sentiment of that word. \n",
    "When we only use the clossed-class words we also get similair results. This tells us that the most information about the sentiment of the review is captured in those clossed-class words.\n",
    "This makes sense because adjectives (such as bad, good, excellent etc..) intuitivally seems the most important indicator of the sentiment. \n",
    "\n",
    "Using features with lower dimensionality in this case seems to not increase performance, but might improve the speed of the computation, which can be important. \n",
    "\n",
    "One big limitation of the techniques used is that they all do not make use (or only a little in case of ngrams)of sentence structure in the review, this is inherent when using a form of bag of words.\n",
    "Lastly SVM does not scale with datasize, so svm may not be computational feasable for very large corpera which are widely used nowadays. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwaKwfWQhRk_"
   },
   "source": [
    "\n",
    "# Submission \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "aOUeaET5ijk-"
   },
   "outputs": [],
   "source": [
    "# Write your names and student numbers here:\n",
    "# Wouter Zwerink #11248122\n",
    "# Gerson Foks #12589845"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A9K-H6Tii3X"
   },
   "source": [
    "**That's it!**\n",
    "\n",
    "- Check if you answered all questions fully and correctly. \n",
    "- Download your completed notebook using `File -> Download .ipynb` \n",
    "- Check if your answers are all included in the file you submit.\n",
    "- Submit your .ipynb file via *Canvas*. One submission per group. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uhU_tk-BOaXb"
   ],
   "name": "NLP1 2020 Practical 1 (student version)",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
